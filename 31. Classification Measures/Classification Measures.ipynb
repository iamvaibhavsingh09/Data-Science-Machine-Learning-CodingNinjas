{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjs6Q6jTQTw7"
   },
   "source": [
    "# **Classification Measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbUTUkqtPxeQ"
   },
   "source": [
    "In case of regression, when we wanted to find how good or bad our model performed, we found out the score $R^2$, using the coefficient of determination.\n",
    "In this lecture, we shall explore the various methods for **Classification Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y7EjYgfQJvp"
   },
   "source": [
    "Perhaps the most obvious solution that comes in mind is the use of **accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y31XHvyYQZ9c"
   },
   "source": [
    "### **Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQksPVMoQZ7E"
   },
   "source": [
    "Accuracy is either the fraction or the count of correct predictions made.\n",
    "If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is less than 1.0.\n",
    "Lets take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYR17CYlRKD0"
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY05ILW4QZ4p"
   },
   "source": [
    "It can be easily seen, that among the 4 values above, only 2 have been predicted correctly. Hence, we have achieved an accuracy of 0.5.\n",
    "\n",
    "We may also say that, we have achieved 50% accuracy. \n",
    "\n",
    "Sklearn offers a function to find the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NI13BZx1RiiX",
    "outputId": "7bb324e1-d133-41f7-ee8a-c526227f42c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "print(\"Score :\", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdJUmn9xRr7X",
    "outputId": "9df9024b-f30b-4023-cab6-bc066a8ad5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage : 50.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage :\", accuracy_score(y_true, y_pred) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6KHFbohQZ2M"
   },
   "source": [
    "If you want to find out the exact number of correct predictions, you may use the normalise parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhdvX-I6UyNC",
    "outputId": "bb1597e6-dfd1-417c-91c8-5a4950093105"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfDT_V_B4BRx"
   },
   "source": [
    "**Why accuracy isn't always the best option?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpDlsMlcQZz2"
   },
   "source": [
    "Suppose you have a highly skewed dataset. Suppose a dataset has 100 datapoints, with 95 zeros and 5 ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUNTKKF5V9XN"
   },
   "outputs": [],
   "source": [
    "y_true = [0]*95 + [1]*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwLCMZHMQZxP"
   },
   "source": [
    "Now, lets assume that our ML model always predicts the answer to be 0, no matter what input it has been given. Hence, it will give us y_pred as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BL_kLw2vWOa9"
   },
   "outputs": [],
   "source": [
    "y_pred = [0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va2G_E0CWadk",
    "outputId": "65509ec4-8010-4e8f-ab12-1bf6f310b94f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"Score :\", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-EpFtPbWSNM"
   },
   "source": [
    "We are getting a 95% accuracy, which suggest that our model is quite good, but we know that this isn't the case, because our model will predict 0 for any value given, and hence is a very bad model.\n",
    "\n",
    "This is why accuracy isn't always the most elegant way to find how good our ML model actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AW7R6fgbQZuo"
   },
   "source": [
    "### **Confusion Matrix**\n",
    "Confusion matrix is useful in that we can assess how many predictions the model got right, and we understand that the model is performing in this particular way so we can think about how we can further improve our model.</p>\n",
    "<p>There are some terms that one must know regarding confusion matrices.</p>\n",
    "<ol>\n",
    "    <li><b>True Positives:</b> This is the number of samples predicted positive which were actually positive.</li>\n",
    "    <li><b>True Negatives:</b> This is the number of samples predicted negative which were actually negative.</li>\n",
    "    <li><b>False Positives:</b> This is the number of samples predicted positive which were <b>not</b> actually positive.</li>\n",
    "    <li><b>False Negatives:</b> This is the number of samples predicted negative which were <b>not</b> actually negative.</li>\n",
    "</ol>\n",
    "<p>In the case of multi-class classification, however, the confusion matrix shows the number of samples predicted correctly and wrongly for each class instead of true positives etc.</p>    \n",
    "\n",
    "Lets see how confusion matrix helps us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ok5Fpwx7XkCN",
    "outputId": "54b21817-886d-44c4-eba2-d4405473bf02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95  0]\n",
      " [ 5  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtF1W2EpQZfK"
   },
   "source": [
    "So, from the above matrix we can easily understand something is wrong with our model as it is never predicting Class 1.\n",
    "\n",
    "Let's look at another example: **The Iris Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dORgeE_3Yics"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1iYZyf2YtVJ"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mas5OlFnYu2z"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DTWJOPqYwAZ"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver = 'liblinear')\n",
    "clf.fit(x_train, y_train)\n",
    "y_train_pred = clf.predict(x_train)\n",
    "y_test_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFlOeVgrY1Yj",
    "outputId": "7b585da8-6b67-4c98-f4a1-915efb0519ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37,  0,  0],\n",
       "       [ 0, 28,  6],\n",
       "       [ 0,  0, 41]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVsnCQryY24A"
   },
   "source": [
    "**Inference:** From the above confusion matrix formed on the training data, we notice the following things.\n",
    "\n",
    "Class 0 had 37 datapoints, and all of them were labelled correctly.\n",
    "\n",
    "Class 1 had 34 datapoints, out of which 28 were predicted correctly and 6 were mislabelled as \"Class 2\".\n",
    "\n",
    "Class 2 had 41 datapoints, and all of them were labelled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOFrT-RaY3P7",
    "outputId": "d8a53cb6-c4bd-4d6e-9a90-33070c14b031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0, 10,  6],\n",
       "       [ 0,  0,  9]])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23NSs9nfY4FQ"
   },
   "source": [
    "You can draw your own inferences for the testing data in similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgOH2Oy9bCNp"
   },
   "source": [
    "### **Classification Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eLkUhErbC0L"
   },
   "source": [
    "There are measures other than the confusion matrix which can help achieve better understanding and analysis of our model and its performance. We talk about two particular measures here - precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnyj7sfXbDed"
   },
   "source": [
    "**Precision**\n",
    "\n",
    "Precision defines the percentage of samples with a certain predicted class label actually belonging to that class label. \n",
    "\n",
    "**Recall**\n",
    "\n",
    "Recall defines the percentage of samples of a certain class which were correctly predicted as belonging to that class.\n",
    "\n",
    "Note that precision and recall will be defined per class label, not for the dataset as a whole. \n",
    "\n",
    "However, how do we choose between precision and recall? Which one is a better metric - precision or recall? Turns out, we can use a better metric which combines both of these - **the F1 score.** \n",
    "\n",
    "**F1 Score**\n",
    "\n",
    "The F1 score is defined as the harmonic mean of precision and recall, and is a far better indicator of model performance than precision and recall individually (usually)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIl5T6pnbDbz"
   },
   "source": [
    "Lets look at the report for the Iris Dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaadAChMcWtx",
    "outputId": "13b46fcc-b46e-4ee4-f984-50dae7123c78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        37\n",
      "           1       1.00      0.82      0.90        34\n",
      "           2       0.87      1.00      0.93        41\n",
      "\n",
      "    accuracy                           0.95       112\n",
      "   macro avg       0.96      0.94      0.95       112\n",
      "weighted avg       0.95      0.95      0.95       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnoPm6fVgZ7m"
   },
   "source": [
    "Lets look deeper into the report of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFOL2wSWgRLm",
    "outputId": "dd4b52ac-7495-44cf-c1c8-02585209bb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 10  6]\n",
      " [ 0  0  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.62      0.77        16\n",
      "           2       0.60      1.00      0.75         9\n",
      "\n",
      "    accuracy                           0.84        38\n",
      "   macro avg       0.87      0.88      0.84        38\n",
      "weighted avg       0.91      0.84      0.84        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCTUaHtifVQQ"
   },
   "source": [
    "We predicted 13 values to belong to **Class 0**, out of which all 13 were labelled correctly by us, hence precision is $\\frac{13}{13} = 1$.\n",
    "\n",
    "We had 13 values in **Class 0**, and we recalled all of them, hence recall is $\\frac{13}{13} = 1$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLE2i2CWhvkI"
   },
   "source": [
    "We predicted 10 values to belong to **Class 1**, out of which all 10 were labelled correctly by us, hence precision is $\\frac{10}{10} = 1$.\n",
    "\n",
    "We had 16 values in **Class 1**, and we recalled 10 of them, hence recall is $\\frac{10}{16} = 0.62$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2LgSQ03hvXt"
   },
   "source": [
    "We predicted 15 values to belong to Class 2, out of which 9 were labelled correctly by us, hence precision is $\\frac{9}{15} = 0.60$.\n",
    "\n",
    "We had 9 values in Class 2, and we recalled all of them, hence recall is $\\frac{9}{9} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJGE6wzQjaNx"
   },
   "source": [
    "## **Some other metrics in Sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3AKo3HPjaFG"
   },
   "source": [
    "### **Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws0ny49zjqDz"
   },
   "source": [
    "The precision is the ratio $t_p / (t_p + f_p)$ where $t_p$ is the number of true positives and $f_p$ the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The best value is 1 and the worst value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6ruJFpFjmEg",
    "outputId": "da637dfd-8665-4586-ed09-d6cca6cced72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_xHt8XjjZ5y"
   },
   "source": [
    "There a a few more options for the 'average' parameter. You may refer to the documentation.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Classification Measures Notes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
