{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bgcmt6DCTUhB"
   },
   "source": [
    "# **Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wz7q9ZATUe2"
   },
   "source": [
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6me44JUWf-2"
   },
   "source": [
    "**What will you learn?**\n",
    "\n",
    "1. **Bayes Theorem**: An introduction\n",
    "2. **Naive Assumption**: Calculation of probability for Naive Bayes Algo.\n",
    "3. **Implementation with Discrete Data**: Theory and code structure.\n",
    "4. **Laplace Correction**: Handling zero probabilities.\n",
    "5. **Self Implementation**: Code for Categorical Input.\n",
    "6. **Scikit Learn Implementation**: GaussianNB, MultinomialNB, CategoricalNB and BernoulliNB.\n",
    "7. **Applications of Naive Bayes**: When to use which type of Naive algorithm.\n",
    "8. **Tips to Improve your Model**: How to process data\n",
    "9. **Advantages and Disadvantages**: Pros and cons of using naive bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd5mREPNTUcb"
   },
   "source": [
    "## **Introduction to Bayes Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPQPp7QETUaL"
   },
   "source": [
    "Bayes Theorem defines probability of an event based on the prior knowledge of factors that might be related to an event.\n",
    "\n",
    "$$ P(A\\;|\\;B) = \\frac{P(B \\;|\\;A) \\; * P(A)}{P(B)} $$\n",
    "\n",
    "where $A$ and $B$ are events and $P(B) \\neq 0$\n",
    "\n",
    "Here, a few points to note are :\n",
    "\n",
    "$\\qquad P(A\\;|\\;B)$ is a conditional probability: the likelihood of event $A$ occuring given that event $B$ has occurred.\n",
    "\n",
    "$\\qquad P(B\\;|\\;A)$ is also a conditional probability: the likelihood of event B occuring given that event $A$ has occurred.\n",
    "\n",
    "$\\qquad P(A)$ and $P(B)$ are the probabilities of observing A and B independently of each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6beXq1iuYgty"
   },
   "source": [
    "### **How Naive Bayes Works?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWNGf3SWYkDm"
   },
   "source": [
    "Let’s understand it by using an example. Below we have a training data set of weather and corresponding target variable ‘Play’ (suggesting possibilities of playing). Now, we need to classify whether players will play or not based on weather. Let’s follow the following steps to perform it.\n",
    "\n",
    "**Step 1:** Convert the data set into a frequency table\n",
    "\n",
    "**Step 2:** Create a Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing = 0.64.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/bayes_41-7363.png\">\n",
    "(In this image, a blank cell represents '0')\n",
    "\n",
    "**Step 3:** Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.\n",
    "\n",
    "**Problem:** Players will play if weather is sunny. Is this statement is correct?\n",
    "\n",
    "We can solve it using above discussed method of posterior probability.\n",
    "\n",
    "$$P(Yes \\;| \\;Sunny) = \\frac{P( Sunny\\; | \\;Yes) * P(Yes)}{P (Sunny)}$$\n",
    "\n",
    "Here we have $P (Sunny |Yes) = 3/9 = 0.33$, $P(Sunny) = 5/14 = 0.36$, $P( Yes)= 9/14 = 0.64$\n",
    "\n",
    "Now, \n",
    "\n",
    "$$P (Yes \\;|\\; Sunny) = \\frac{0.33 * 0.64} { 0.36 }= 0.59$$\n",
    "which is a high probability.\n",
    "\n",
    "Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9vgE-5HTUXt"
   },
   "source": [
    "Now, basically for a data point $x_i$, we have to predict the class that the current output $y$ belongs to. Assume, there are total $j$ number of classes for output.\n",
    "Then,\n",
    "\n",
    "$P(y=c_1 \\;| \\; x=x_i)$ : tells us that for given input $x_i$ what is the probability that $y$ is $c_1$.\n",
    "\n",
    "$P(y=c_2 \\;| \\; x=x_i)$ : tells us that for given input $x_i$ what is the probability that $y$ is $c_2$.\n",
    "\n",
    "and so on till $c_j$.\n",
    "\n",
    "\n",
    "Out of all these probabilities calculations, $y$ belongs to that particular class which has maximum probability.\n",
    "\n",
    "We will be using Bayes theorem for doing these probability calculations. The formula is given as:\n",
    "\n",
    "$$P(y = c_j \\; | \\; x = x_i) = \\frac{P(x = x_i \\; | \\; y = c_j) * P(y = c_j)}{P(x = x_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjl5eOBbTUVV"
   },
   "source": [
    "This gives us the probability that the output belongs to $j^{th}$ class for the current values of data point $(x_i)$.\n",
    "\n",
    "\n",
    "Since for all the classes 1, 2,..., j the denominator will have the same value, so we can ignore this while doing comparison (ultimately, we have to find the max). Hence, we obtain the given formula to calculate probabilities.\n",
    "\n",
    "$$P'(y = c_j \\; | \\; x = x_i) = P(x = x_i \\; | \\; y = c_j) * P(y = c_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsouMKZUTUQW"
   },
   "source": [
    "## **Naive Assumption**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JkcoJBjTUOO"
   },
   "source": [
    "The estimate for probability $P(y=c_j)$, can be done directly from the number of training points. <br/>\n",
    "Suppose there are 100 training points and 3 output classes, 10 belong to class $C_1$, 30 belong to class $C_2$ and remaining 60 belong to class $C_3$. <br/>\n",
    "The estimate values of class probabilities will be: <br/>\n",
    "$P(y = C_1) = 10/100 = 0.1$ <br/>\n",
    "$P(y = C_2) = 30/100 = 0.3$ <br/>\n",
    "$P(y = C_3) = 60/100 = 0.6$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThAJoaseTULu"
   },
   "source": [
    "To make the probability estimate for $P(x=x_i \\; | \\; y=c_j)$, naive bayes classification algorithm assumes <b>all the features to be independent</b>. \n",
    "\n",
    "So, we can calculate this by individually multiplying the probabilities obtained for all these features (assuming features to be independent), for the output of $j^{th}$ class.\n",
    "\n",
    "$$P(x=x_i|y=c_j) = P(x=x_i^1 \\; | \\; y=c_j) * P(x=x_i^2 \\; | \\; y=c_j) * .... * P(x=x_i^n \\; | \\; y=c_j)$$\n",
    "\n",
    "Here, $x_i^1$ denotes the value of 1st feature of $i^{th}$ data point and $x=x_i^n$ denotes the value $n^{th}$ feature of the $i^{th}$ data point.\n",
    "\n",
    "\n",
    "After taking up the naive assumption, we can easily calculate the individual probabilites and then by simply multiplying the result calculate the final probability $P'$.\n",
    "\n",
    "$$ P'(y = c_j \\; | \\; x = x_i) = \\prod_k P(x = x_i^k \\; | \\; y = c_j) * P(y = c_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KLrv2nbE_3P"
   },
   "source": [
    "Finding the second term in the above formula is pretty straight forward, we shall simply divide the total training data belonging to class $c_j$, with total training data. Let's have a look at the first term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipSGRXKuGZY3"
   },
   "source": [
    "## **Implementation with Discrete Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXHdEKzQTUJX"
   },
   "source": [
    "Now, let's see how we can find $P(x = x_i^k \\; | \\; y = c_j)$ for any one class.\n",
    "\n",
    "Let's assume our selected class to be $c$, and the value of the input feature to be $x^j$, so the probability $P(x = x^j \\; | \\; y = c)$ is given by\n",
    "\n",
    "$$ P(x = x^j \\; | \\; y = c) = \\frac{Count \\; of \\; Training \\; Data(x = x^j \\; and \\; y = c)}{Count \\; of \\; Training \\; Data(y = c)}$$\n",
    "\n",
    "\n",
    "Let's look at the previous example.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/bayes_41-7363.png\">\n",
    "\n",
    "Here, let's find the probability $P(x = Sunny \\; | \\; y = No)$\n",
    "\n",
    "$$P(x = Sunny \\; | \\; y = No) = \\frac{Count \\; of \\; Training \\; Data(x = Sunny \\; and \\; y = c)}{Count \\; of \\; Training \\; Data(y = No)}$$\n",
    "\n",
    "$$= \\frac{2}{5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fFk9PHoBmA7"
   },
   "source": [
    "The above example considers only one feature for our input. To maintain computations for multi featured input, we shall use dictionaries while we write the code. Let us look at the dictionary structure.\n",
    "\n",
    "We will implement a multi level dictionary. At the first level, we will store the classes $(c_1, c_2, c_3, ...., c_j)$ as keys, to which the data belongs to.\n",
    "\n",
    "For each class key we will store another dictionary (second level), where the keys will be the features, ($x_j^1, x_j^2, x_j^3, ...., x_j^n$), where n is the number of features.\n",
    "\n",
    "For each feature, we will store another dictionary (third level), where the keys will be all the possible values that feature can take. The keys of this dictionary will store the corresponding count.\n",
    "\n",
    "**Note:** Apart from storing the feature dictionaries, the top level dictionary of each class will store one extra key, where the value would be the frequency  of occurance (total count) of that particular class.\n",
    "\n",
    "Below is the diagramatic structue of the same.\n",
    "\n",
    "<img src = \"https://files.codingninjas.in/dict-7460.jpg\" width = \"650\">\n",
    "\n",
    "where $c_1, c_2, ...., c_j$ represent the classes, $x^1, x^2, x^3...., x^n$ represent the features and $l_1, l_2, l_3, ...., l_i$ represent the possible labels of each feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ym8cHAnKQyu"
   },
   "source": [
    "## **Laplace Correction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmreGfuGKf50"
   },
   "source": [
    "Let’s consider the following situation: you’ve trained a Naive Bayes algorithm to differentiate between spam and not spam mails. What happens if the word “Casino” doesn’t show up in your training data set, but appears in a test sample?\n",
    "\n",
    "Well, your algorithm has never seen it before, so it sets the probability that <b>\"Casino\" appears in a spam document</b> to <b>0</b>; So every time this word appears in the test data , you will try hard (it has P = 0) to mark it as not spam just because you have not seen that word in the spam part of training data.This will make the model very less efficient and thus we want to minimise it. We want to keep in mind the possibility of any word we have not seen (or for that matter seen in the not-spam part of training data), may have a above-zero probability of being a word used in spam mails. The same is true for each word to be a part of not-spam mails. \n",
    "\n",
    "To avoid such issues with unseen values for features, as well as to combat overfitting to the data set, we pretend as if we’ve seen each word 1 (or k, if you’re smoothing by k) time more than we’ve actually seen it, and adjust the denominator of our frequency divisions by the size of the overall vocabulary to account for the “pretence”, which actually works well in practice.\n",
    "\n",
    "If you take smoothing factor k equal to 1, it becomes Laplace correction.\n",
    "The equations below show Laplace correction for the example taken.\n",
    "\n",
    "Without correction : \n",
    "\n",
    "$$ P(w_i \\; | \\; c_j) = \\frac{count(w_i, c_j)}{\\sum_w count(w, c_j)}$$\n",
    "\n",
    "This is the fraction of times the word $w_i$ appears among all words in documents of topic $c_j$.\n",
    "\n",
    "With correction :\n",
    "\n",
    "$$ P(w_i \\; | \\; c) = \\frac{count(w_i, c_j) + 1}{(\\sum_w count(w, c_j)) + |V|} $$\n",
    "\n",
    "where $|V|$ is the number of all possible words in mails with topic $c_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElGXmAy3GmeW"
   },
   "source": [
    "## **Self Implementation of Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJHYXYV2GmJh"
   },
   "source": [
    "We will work with the Iris Dataset for this example. It is a continuous data, we shall convert it into categorigal(discrete) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAS7WmW_IUl1"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znwEnFlAIVJI"
   },
   "outputs": [],
   "source": [
    "# This function creates the dictionary as discussed in implementation theory.\n",
    "def fit(X_train, Y_train):\n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1]\n",
    "        result[current_class][\"total_count\"] = len(Y_train_current)\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] = {}\n",
    "            all_possible_values = set(X_train[:, j - 1])\n",
    "            for current_value in all_possible_values:\n",
    "                result[current_class][j][current_value] = (X_train_current[:, j - 1] == current_value).sum()\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9HFHeNFKhMc"
   },
   "outputs": [],
   "source": [
    "# Function to find probability of current class\n",
    "def probability(dictionary, x, current_class):\n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    num_features = len(dictionary[current_class].keys()) - 1;\n",
    "    for j in range(1, num_features + 1):\n",
    "        xj = x[j - 1]\n",
    "        count_current_class_with_value_xj = dictionary[current_class][j][xj] + 1\n",
    "        count_current_class = dictionary[current_class][\"total_count\"] + len(dictionary[current_class][j].keys())\n",
    "        current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)           # By taking log values, we prevent multiplication with a 0 probability\n",
    "        output = output + current_xj_probablity\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9HqtuykKjkL"
   },
   "outputs": [],
   "source": [
    "# This function predicts what class a single point belongs to\n",
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):             # Obtaining the class with higher probability\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BIKM0T7Klsl"
   },
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gZs8oqHMf2s"
   },
   "outputs": [],
   "source": [
    "# Converting Iris Data into Labelled data\n",
    "def makeLabelled(column):\n",
    "    second_limit = column.mean()\n",
    "    first_limit = 0.5 * second_limit\n",
    "    third_limit = 1.5 * second_limit\n",
    "    for i in range (0,len(column)):\n",
    "        if (column[i] < first_limit):\n",
    "            column[i] = 0\n",
    "        elif (column[i] < second_limit):\n",
    "            column[i] = 1\n",
    "        elif(column[i] < third_limit):\n",
    "            column[i] = 2\n",
    "        else:\n",
    "            column[i] = 3\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErDh_TpZMf0V"
   },
   "outputs": [],
   "source": [
    "# Importing Iris\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SX3ak3TMfsJ"
   },
   "outputs": [],
   "source": [
    "for i in range(0,X.shape[-1]):\n",
    "    X[:,i] = makeLabelled(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veu2aJuiMfqD"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhJzRyH5MtZ1"
   },
   "outputs": [],
   "source": [
    "dictionary = fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwJOYaPAMtXY"
   },
   "outputs": [],
   "source": [
    "Y_pred = predict(dictionary, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUUsRwDbMtPl",
    "outputId": "f0816b7e-a615-4258-848a-c4a9c49d369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       0.94      1.00      0.97        16\n",
      "           2       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.96      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "Confusion Matrix\n",
      "[[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  1  8]]\n",
      "\n",
      "Accuracy Score\n",
      "97.36842105263158%\n"
     ]
    }
   ],
   "source": [
    "# Various metrics for understanding how well our model has performed.\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print()\n",
    "print(\"Accuracy Score\")\n",
    "print(accuracy_score(Y_test, Y_pred) * 100, \"%\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnBemRt8QT4C"
   },
   "source": [
    "## **Implementation with Continous Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSMTga2qQT0o"
   },
   "source": [
    "For continous data, we need a **probability distribution**. For most of the cases, **Gaussian Distribution** is used.\n",
    "\n",
    "$$ f(x \\; | \\; \\mu, \\sigma^2) = \\frac{1}{ \\sqrt {2 \\pi \\sigma^2}} * e^{- \\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "where \n",
    "\n",
    "$\\quad \\mu$ is the mean of the distribution\n",
    "\n",
    "$\\quad \\sigma$ is the standard deviation, and \n",
    "\n",
    "$\\quad \\mu^2$ is the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CINT034CQTxP"
   },
   "source": [
    "Other types of classifiers are **Bernoulli, Multinomial**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKZVv-vNQTt3"
   },
   "source": [
    "## **Implementation of Naive Bayes using Scikit Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QX15uh6U1DJ"
   },
   "source": [
    "There are three types of Naive Bayes models under the scikit-learn library:\n",
    "\n",
    "**Gaussian:** It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "**Multinomial:** It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number $x_i$ is observed over the n trials”.\n",
    "\n",
    "**Bernoulli:** The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjy4_TgBhSOw"
   },
   "source": [
    "### **Iris Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXnH1bN9U0_l"
   },
   "source": [
    "If you remember, for the model we implemented ourselves, we converted our data to labelled data.\n",
    "\n",
    "For such data, the best model present in Sklearn is **CategoricalNB**. Lets have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuUuJCBugP1h"
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sok_eAn8gPzI"
   },
   "outputs": [],
   "source": [
    "for i in range(0,X.shape[-1]):\n",
    "    X[:,i] = makeLabelled(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWDURqO_gPxG"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8bEV4SegPuo",
    "outputId": "1d164695-4936-4d8b-d27c-878dbd3c9ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf = CategoricalNB()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBcQvxShgPsR"
   },
   "outputs": [],
   "source": [
    "Y_pred_CategoricalNB = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fpplpC-gPqB",
    "outputId": "1082517e-d6bb-4084-ca80-85b35b61a379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       0.94      1.00      0.97        16\n",
      "           2       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.96      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "Confusion Matrix\n",
      "[[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  1  8]]\n",
      "\n",
      "Accuracy Score\n",
      "97.36842105263158%\n"
     ]
    }
   ],
   "source": [
    "# Various metrics for understanding how well our model has performed.\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(Y_test, Y_pred_CategoricalNB))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test, Y_pred_CategoricalNB))\n",
    "print()\n",
    "print(\"Accuracy Score\")\n",
    "print(accuracy_score(Y_test, Y_pred_CategoricalNB) * 100, \"%\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsb0k6cngtuL"
   },
   "source": [
    "It is observed that our implementation for Categorical data gives us a similar result to sklearn's CategoricalNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5yfX0PlgaJO"
   },
   "source": [
    "Now, let's have a look at the **GaussianNB**. Here, we shall use the **original values (continuous)** of the Iris Dataset, instead of using the labelled data we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YijQLl0CVELh"
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waKAo4vdWBxy"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG59U0jtVEGf",
    "outputId": "aacdb54a-1416-415a-bb67-c57dc9567e0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = naive_bayes.GaussianNB() # GAUSSIAN NAIVE BAYES CLASSIFIER\n",
    "gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgGu8U19VEB8"
   },
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyEQX92qVD_b",
    "outputId": "36081749-3a6b-4ca3-9148-7f6793ed58a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n",
      "Confusion Matrix\n",
      "[[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0  9]]\n",
      "\n",
      "Accuracy Score\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Various metrics for understanding how well the model has performed.\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test,y_pred))\n",
    "print()\n",
    "print(\"Accuracy Score\")\n",
    "print(accuracy_score(Y_test,y_pred) * 100, \"%\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBR40_57bx0s"
   },
   "source": [
    "Lets take another dataset, **Wine Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJvgEmHQc5jt"
   },
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NZ7Yjn_c6kj"
   },
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(wine.data, wine.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLu6PveJdEf9",
    "outputId": "2de3c93c-3a07-47b5-8222-427949096af1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeB-UlTUdHKJ"
   },
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is6DZ2judWgc",
    "outputId": "a5cdd048-2d46-49d2-9f25-216b06675ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        19\n",
      "           1       1.00      0.86      0.93        22\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.94      0.95      0.95        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n",
      "Confusion Matrix\n",
      "[[19  0  0]\n",
      " [ 2 19  1]\n",
      " [ 0  0 13]]\n",
      "\n",
      "Accuracy Score\n",
      "94.44444444444444%\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Accuracy Score\")\n",
    "print(accuracy_score(y_test, y_pred) * 100, \"%\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyG5Gg96l5Vg"
   },
   "source": [
    "## **Some Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6VzaPEYl5K2"
   },
   "source": [
    "**Real time Prediction:** Naive Bayes is an eager learning classifier and it is sure fast. Thus, it can be used for making predictions in real time.\n",
    "Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n",
    "\n",
    "**Text Classification / Spam Filtering / Sentiment Analysis:** Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "\n",
    "**Recommendation System:** Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAUTVVxZVNDt"
   },
   "source": [
    "## **Tips to improve the power of Naive Bayes Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOXq4MX6VM4X"
   },
   "source": [
    "1. If continuous features do not have normal distribution, we should use transformation or different methods to convert it in normal distribution.\n",
    "2. If test data set has zero frequency issue, apply smoothing techniques “Laplace Correction” to predict the class of test data set.\n",
    "3. Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over inflating importance.\n",
    "4. You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, “ensembling, boosting, bagging” won’t help since their purpose is to reduce variance. Naive Bayes has no variance to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6W5gEYyfTTnq"
   },
   "source": [
    "## **Advantages of Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74N9NmwMlhH6"
   },
   "source": [
    "1. It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
    "2. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "3. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpSlMNHMlnTw"
   },
   "source": [
    "## **Disadvantages of Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahDk6mEOln0I"
   },
   "source": [
    "1. If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace correction.\n",
    "2. Naive Bayes is also known as a bad estimator, so the probability outputs from predict probability are not to be taken too seriously.\n",
    "3. Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuDFsDR7WnNi"
   },
   "source": [
    "## **Your Next Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWBGtjSTWqiR"
   },
   "source": [
    "We have implemented Naive Bayes for discrete data. We have also seen the implementation of Sklearn to predict continuos data.\n",
    "\n",
    "Try and write a code to implement Naive Bayes to predict continous data on your own. You may use Gaussian Naive Bayes, or Multinomial Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Naive Bayes Notes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
