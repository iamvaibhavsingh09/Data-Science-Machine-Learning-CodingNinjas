{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkoC-f1RK6Qe"
   },
   "source": [
    "# **Random Forests**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfRusHwkyiYv"
   },
   "source": [
    "**What will you learn?**\n",
    "1. **Disadvantages of Decision Trees:** The need of Random Forests\n",
    "2. **Random Forest:** Basic intuition and introduction\n",
    "3. **Data Bagging:** A powerful ensemble method\n",
    "4. **Extra Trees:** Extremely Randomised Trees\n",
    "5. **Implementation using Sklearn:** RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIF5k6Jgyidi"
   },
   "source": [
    "## **Disadvantages of Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9RBevRgyigE"
   },
   "source": [
    "As we have already seen in decision trees, **overfitting** is a major problem. We used pruning to reduce overfitting and got a good decision tree.\n",
    "As decision trees keep expanding till they run out of features or till they find a pure node, therefore there is a very high chance of overfitting on the training data. Decision tree will split on the nodes that are not actually important if it is favourable. This causes decision trees to perform perfectly on training data but sometimes fail on testing data. Pruning definitely helps to some extent but even it does not consider how important a feature is. Decision tree may be going to a direction on the basis of some useless features at the starting levels of the tree and thus it effects the decision. Pruning may improve the error but error will still remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMFbrOZGyiic"
   },
   "source": [
    "For example, in our previous example of predicting whether a candidate gets an interview call based upon their resume, we also included features like: resume has a picture of the candidate, colours used in the resume and number of pages resume has. These features mentioned are comparitively less useful as compared to our earlier features (which were projects, college and internships). But there can be a case where our data-set has positive results corresponding to blue colour of resume and resume having the applicant's picture. Therefore, our decision tree may have these as deciding features and gives true as soon as these two are present in a particular resume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QZ4JCY2yikv"
   },
   "source": [
    "##**Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73R8hu1f1Bq3"
   },
   "source": [
    "Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
    "\n",
    "Random forest is a way to reduce overfitting in decision trees and it can also be used to find importance of features we are using. \n",
    "Each decision tree built will have a randomly selected set of features and randomly selected set of data points. Number of features in each decision tree in the forest will be less than the total number of features we have in our dataset. So if we have a feature 'A', it may appear in some of the decision trees of the forest and not in others. Duplication is generally allowed in selecting the data-points from our data-set for the decision trees in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IavoGSNJ1Btj"
   },
   "source": [
    "<img src=\"https://files.codingninjas.in/rf1-7334.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10hdhyqy1BwY"
   },
   "source": [
    "As shown above, some of the trees have features F1 and F2 but not F3. There is no point of repeating a feature in a decision tree of the forest but as shown data point may be duplicate. This randomness in selecting the features and data-points helps in reducing overfitting. Note that we will make multiple decision trees so that there is very less chance of a feature/data-point getting missed out.\n",
    "\n",
    "Random forest consists of many decision tress. Final answer of prediction is the majority of the answers from decision trees of the forest.\n",
    "\n",
    "There are a few important ways to do so, let's have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFemuN0c2Tvk"
   },
   "source": [
    "### **Data Bagging and Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJe5tnCC2Txu"
   },
   "source": [
    "**B**ootstrap **Agg**regation or Bagging is a simple but very powerful ensemble method.\n",
    "\n",
    "An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model . \n",
    "\n",
    "Bagging is used when our goal is to reduce the variance of a decision tree. It is a general purpose procedure for reducing the variance of a predictive model. When applied to trees the basic idea is to grow multiple trees which are then combined to give a single prediction. Combining multiple trees helps in improving precision and accuracy at the expense of interpretation. <br>\n",
    "In bagging, we take multiple smaller data-sets in which we also allow repetition of data points and randomly select some features. Bagging is generally done in reference of data-points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jN1EawG2T0k"
   },
   "source": [
    "<img src=\"https://files.codingninjas.in/bagging-7335.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLKKg99E2T3n"
   },
   "source": [
    "As shown in the diagram above we have created multiple data-sets from the original dataset shown as D1, D2 etc. Classifiers C1, C2 etc are actually individual trees in our forest. To find the final answer we just take majority of the answers given by these trees.<br> These smaller data-sets are obtained by choosing the data-points and the features in the following manner:\n",
    "\n",
    "1.  <b>Features</b> are selected at random <b>without</b> repetiton<br>\n",
    "2.  <b>Data-points</b> are selected at random <b>with</b> repetition (which is actually bagging)\n",
    "\n",
    "\n",
    "While doing bagging, we must be sure that no data point is left out. Increasing the number of trees in the random forest significantly reduces the chances of missing out on any data points. Same goes for the features. As already discussed, selecting features in random forest helps us in knowing the relative importance of each feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tQTwOYu6Hqe"
   },
   "source": [
    "### **Extra Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUP4N3C66MnN"
   },
   "source": [
    "The Extra-Tree method stands for <b>ext</b>remely <b>ra</b>ndomized <b>trees</b>. With respect to random forests, the method drops the idea of using bootstrap copies of the learning sample, and instead of trying to find an optimal cut-point for each one of the K randomly chosen features at each node, it selects a cut-point at random.\n",
    "<br>In the implementation of Random Forest, we randomly select some features from the main data-set. Then for these randomly selected features we make a decision tree. The decision tree is made by calculating which feature should be selected to split at a particular node. The cost of choosing each feature was calculated and the feature which gave the least cost was selected. Multiple trees were made using the same approach to form a forest.\n",
    "<br>In the Extra Trees approach, we do not choose the features randomly to form a tree. We take all the features to form a tree. This is the first difference. The second one is in selecting the feature to split the data points at each node. Rather than considering the cost due to taking a certain feature to split Extra Trees just pick a feature at random. So in this case, any two trees will be different in terms of the feature selected to partition the data-points at each node. It is quite possible in both the approaches discussed that there exists a pair of trees that are exactly same.\n",
    "<br>We can combine Extra Trees approach with Random forests as well. This can be done by selecting randomly some features as Random forest does and then applying Extra Trees to this feature subset and make multiple Trees out of the bootstraped data-set formed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ1I0eYv6MjC"
   },
   "source": [
    "<img src=\"https://files.codingninjas.in/extra_trees-7336.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0-NxOl66Mgs"
   },
   "source": [
    "As shown in the image above, the data-set is first converted into several bootstrap data-sets and then Extra Trees are made for each of these bootstrap data-sets.\n",
    "<br><br>We have an inbuilt Classifier for Extra Trees in sklearn as sklearn.tree.ExtraTreeClassifier. More information about it is available at [sklearn.tree.ExtraTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj5R8Smq6MLy"
   },
   "source": [
    "### **Random Forest Using Sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn3Ht66DAvLN"
   },
   "source": [
    "Feature selection is picking only useful features that makes up major contribution in the output.\n",
    "Advantages of feature selection are as follows :\n",
    "\n",
    "1. Reduces Overfitting\n",
    "2. Improves accuracy of the model\n",
    "3. Reduces training time\n",
    "\n",
    "If we have too many irrelevant features, **the accuracy of our classifier decreases.** <br/>\n",
    "Random Forests can be used for determing the importance of each feature and then picking up only important and useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcAwroxJA4-3"
   },
   "source": [
    "**Dataset Used** - Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSjb6uXmAAMC",
    "outputId": "3dbcf563-2d59-4a87-e0a2-afe9892ead57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets,tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGjOtScRA10Q"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "features = iris.feature_names\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69w40480A32z"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MGMysN0BARk"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10000, n_jobs=-1, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z63XoJawBB2_",
    "outputId": "e35aad46-1fb5-4efc-d533-2f46b778b490"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=14, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLHOytzCBDqu",
    "outputId": "dd7e9782-9711-47c1-9fa9-f839db94dbbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "S6LVEFEbBFYy",
    "outputId": "c419d465-1a87-4f1d-9584-822d6243b0dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>petal width (cm)</th>\n",
       "      <td>0.459124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal length (cm)</th>\n",
       "      <td>0.400622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <td>0.118110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <td>0.022145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance\n",
       "petal width (cm)     0.459124\n",
       "petal length (cm)    0.400622\n",
       "sepal length (cm)    0.118110\n",
       "sepal width (cm)     0.022145"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(clf.feature_importances_, index = features, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl6gxalEBLaX"
   },
   "source": [
    "This shows that <b>petal length</b> and <b>petal width</b> are important features as compared to the other two features i.e. <b>sepal length</b> and <b>sepal width</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aIECYrxBH8h"
   },
   "outputs": [],
   "source": [
    "# Making a classifier picking only important features, \n",
    "# picking only those features that have importance value greater than 0.15\n",
    "sfm = SelectFromModel(clf, threshold = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RVNubFdBSha",
    "outputId": "ec1f92d9-14a4-4d26-ebd7-b4ad9df4842f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                                 class_weight=None,\n",
       "                                                 criterion='gini',\n",
       "                                                 max_depth=None,\n",
       "                                                 max_features='auto',\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 max_samples=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_split=2,\n",
       "                                                 min_weight_fraction_leaf=0.0,\n",
       "                                                 n_estimators=10000, n_jobs=-1,\n",
       "                                                 oob_score=False,\n",
       "                                                 random_state=14, verbose=0,\n",
       "                                                 warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=0.15)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGIDgeB-BW3p"
   },
   "outputs": [],
   "source": [
    "# Create a data subset picking only important features out of all the features.\n",
    "X_important_train = sfm.transform(X_train)\n",
    "X_important_test = sfm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOW39GfiBZIf",
    "outputId": "527bcc24-e0f5-4934-9c44-9bec7e7a329e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_important_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzah6FRGgT19",
    "outputId": "627aeb92-2071-46bc-c135-92ac97532d21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_important_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pSVUGbkBeSF"
   },
   "outputs": [],
   "source": [
    "# New random forest classifier with only important features\n",
    "clf_important = RandomForestClassifier(n_estimators=10000, n_jobs=-1, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6DUWnv6BiVA",
    "outputId": "f62b4a5d-c542-4c79-bdb2-10338d8c710a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=14, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_important.fit(X_important_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxnpve7ICkgf",
    "outputId": "419d8873-6112-44e1-848f-b8fde657c9c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_important.score(X_important_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9t-FQolOWqN"
   },
   "source": [
    "As you can see, even after removing two insignificant features from our dataset, we are able to predict the answers with an **increased score**.\n",
    "\n",
    "Thus, using Random Forest, we can easily find out what features to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUHUd4y2R5r5",
    "outputId": "8e42faa7-11c6-4949-c6de-e6bbd5db5dce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the estimators\n",
    "len(clf_important.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWlJ8oc8prhA"
   },
   "source": [
    "It can be seen that this forest contains 10000 Decision Trees."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Random Forest Notes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
